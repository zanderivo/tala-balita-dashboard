{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ADg2kn3ukxT"
      },
      "outputs": [],
      "source": [
        "pip install rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# A). EXTERNAL KNOWLEDGE BASE FETCHER\n",
        "# ==============================================\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "\n",
        "print(\" INITIALIZING KNOWLEDGE ACQUISITION \")\n",
        "\n",
        "# 1. GENERATE STOPWORDS CSV\n",
        "csv_content = \"\"\"word\n",
        "ang,sa,ng,mga,at,ay,na,ni,si,ko,ako,niya,kanya,kanyang,siyang\n",
        "tayo,kami,kayo,sila,ito,iyan,iyon,nito,niyan,noon,ngayon\n",
        "kanilang,kanila,namin,inyo,aming,aking,niyo,nila,kay,kina\n",
        "hindi,dahil,isang,naman,noong,umano,matapos,sinabi,nitong\n",
        "bilang,mula,para,pero,kung,upang,habang,bago,pagkatapos\n",
        "pa,din,rin,lamang,lang,ba,kasi,pala,sana,daw,raw,maging\n",
        "mo,ka,po,opo,may,wala,mas,bawat,iba,lahat,kapwa,tulad\n",
        "gaya,ngunit,subalit,datapwat,kundi,kapag,maski,samantala\n",
        "basta,kaysa,sakali,kabilang,ayon,tungo,laban,ukol,hinggil\n",
        "ano,sino,saan,kailan,paano,bakit,alin,ilan,sang,naging\n",
        "muli,man,kaya,dito,diyan,doon,san,jan,ganito,ganyan\n",
        "\"\"\"\n",
        "with open(\"filipino_stopwords.csv\", \"w\", encoding='utf-8') as f:\n",
        "    words = re.split(r'[,\\n]+', csv_content)\n",
        "    f.write(\"word\\n\")\n",
        "    for w in words:\n",
        "        if w.strip(): f.write(w.strip() + \"\\n\")\n",
        "print(\"‚úÖ 'filipino_stopwords.csv' created.\")\n",
        "\n",
        "# 2. FETCH OFFICIAL PH GEOGRAPHIC CODES (PSGC)\n",
        "# Using the master branch raw link which is the most stable\n",
        "base_url = \"https://raw.githubusercontent.com/clavearnel/philippines-region-province-citymun-brgy/master/json\"\n",
        "\n",
        "def fetch_json(filename):\n",
        "    url = f\"{base_url}/{filename}\"\n",
        "    print(f\"‚¨áÔ∏è Downloading: {filename}...\")\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            # If data is wrapped like {\"records\": [...]}, try to find the list\n",
        "            print(f\"‚ö†Ô∏è Warning: {filename} is a Dictionary, looking for list content...\")\n",
        "            for key, val in data.items():\n",
        "                if isinstance(val, list):\n",
        "                    return val\n",
        "            return [] # Fail safe\n",
        "        elif isinstance(data, list):\n",
        "            # Ensure elements are dicts\n",
        "            if len(data) > 0 and isinstance(data[0], str):\n",
        "                print(f\"‚ùå Error: {filename} contains strings, expected objects.\")\n",
        "                return []\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"‚ùå Error: Unknown format for {filename}\")\n",
        "            return []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error fetching {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "regions = fetch_json(\"refregion.json\")\n",
        "provinces = fetch_json(\"refprovince.json\")\n",
        "cities = fetch_json(\"refcitymun.json\")\n",
        "\n",
        "print(f\"‚úÖ Downloaded: {len(regions)} Regions, {len(provinces)} Provinces, {len(cities)} Cities/Towns\")\n",
        "\n",
        "# 3. BUILD HIERARCHY DB\n",
        "print(\"‚öôÔ∏è Building Relational Hierarchy...\")\n",
        "\n",
        "if len(regions) == 0 or len(cities) == 0:\n",
        "    print(\"‚ö†Ô∏è CRITICAL WARNING: Download failed. Using fallback minimal database.\")\n",
        "    # Fallback so pipeline doesn't crash\n",
        "    location_db = [\n",
        "        {'alias': 'manila', 'official_name': 'Manila', 'province': 'Metro Manila', 'region': 'NCR', 'type': 'Local'},\n",
        "        {'alias': 'quezon city', 'official_name': 'Quezon City', 'province': 'Metro Manila', 'region': 'NCR', 'type': 'Local'},\n",
        "        {'alias': 'cebu', 'official_name': 'Cebu City', 'province': 'Cebu', 'region': 'Region VII', 'type': 'Local'},\n",
        "        {'alias': 'davao', 'official_name': 'Davao City', 'province': 'Davao del Sur', 'region': 'Region XI', 'type': 'Local'}\n",
        "    ]\n",
        "else:\n",
        "    # Build Map\n",
        "    reg_map = {}\n",
        "    for r in regions:\n",
        "        if isinstance(r, dict) and 'regCode' in r:\n",
        "            reg_map[r['regCode']] = r.get('regDesc', 'Unknown')\n",
        "\n",
        "    prov_map = {}\n",
        "    for p in provinces:\n",
        "        if isinstance(p, dict) and 'provCode' in p:\n",
        "            prov_map[p['provCode']] = {\n",
        "                'name': p.get('provDesc', 'Unknown'),\n",
        "                'regCode': p.get('regCode', '00')\n",
        "            }\n",
        "\n",
        "    location_db = []\n",
        "\n",
        "    for city in cities:\n",
        "        if not isinstance(city, dict): continue\n",
        "\n",
        "        name = city.get('citymunDesc', '').title()\n",
        "        prov_code = city.get('provCode')\n",
        "\n",
        "        # Resolve Province & Region\n",
        "        prov_name = \"Metro Manila\" # Default for NCR\n",
        "        reg_name = \"Unknown\"\n",
        "\n",
        "        if prov_code in prov_map:\n",
        "            prov_name = prov_map[prov_code]['name'].title()\n",
        "            reg_code = prov_map[prov_code]['regCode']\n",
        "            if reg_code in reg_map:\n",
        "                reg_name = reg_map[reg_code]\n",
        "        elif city.get('regDesc'):\n",
        "            reg_name = city['regDesc']\n",
        "\n",
        "        location_db.append({\n",
        "            'alias': name.lower(),\n",
        "            'official_name': name,\n",
        "            'province': prov_name,\n",
        "            'region': reg_name,\n",
        "            'type': 'Local'\n",
        "        })\n",
        "\n",
        "# Add Major Countries (International Scope)\n",
        "countries = ['China', 'United States', 'USA', 'America', 'Japan', 'South Korea',\n",
        "             'Singapore', 'Australia', 'Canada', 'Russia', 'Ukraine', 'UK', 'Saudi Arabia']\n",
        "for c in countries:\n",
        "    location_db.append({'alias': c.lower(), 'official_name': c, 'province': 'N/A', 'region': 'International', 'type': 'International'})\n",
        "\n",
        "# Save\n",
        "df_loc = pd.DataFrame(location_db).drop_duplicates(subset='alias')\n",
        "df_loc.to_csv(\"master_locations.csv\", index=False)\n",
        "print(f\"‚úÖ DATABASE READY: 'master_locations.csv' ({len(df_loc)} entries)\")"
      ],
      "metadata": {
        "id": "ge0RFReguluI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# B). TALA CORE PIPELINE (HYBRID + UNSUPERVISED)\n",
        "# ==============================================\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from rapidfuzz import process, fuzz\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#  1. SETUP & LOADING\n",
        "print(\" INITIALIZING TALA \")\n",
        "\n",
        "# Load Knowledge Base\n",
        "try:\n",
        "    loc_df = pd.read_csv(\"master_locations.csv\")\n",
        "    loc_db = loc_df.set_index('alias').T.to_dict()\n",
        "    print(f\"‚úÖ Loaded Knowledge Base: {len(loc_db)} Locations\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: 'master_locations.csv' not found. Run Block 1.\")\n",
        "    loc_db = {}\n",
        "\n",
        "# Load Stopwords\n",
        "try:\n",
        "    stop_df = pd.read_csv(\"filipino_stopwords.csv\")\n",
        "    custom_stops = set(stop_df['word'].str.lower().tolist())\n",
        "except:\n",
        "    custom_stops = set()\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "final_stop_list = list(custom_stops.union(set(stopwords.words('english'))))\n",
        "\n",
        "#  2. FULL INGESTION\n",
        "files = ['train.json', 'test.json', 'validation.json']\n",
        "raw_data = []\n",
        "\n",
        "print(\"\\n PHASE 1: FULL INGESTION \")\n",
        "import random # Ensure random is imported\n",
        "\n",
        "for filename in files:\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            for entry in data: entry['src'] = filename\n",
        "            raw_data.extend(data)\n",
        "            print(f\"‚úÖ Loaded {len(data):,} from {filename}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Missing: {filename}\")\n",
        "\n",
        "# This mixes AbanteTNT with the other outlets before processing starts\n",
        "random.seed(42)\n",
        "random.shuffle(raw_data)\n",
        "\n",
        "df = pd.DataFrame(raw_data)\n",
        "print(f\"üìä Total Dataset: {len(df):,} Articles\")\n",
        "print(\"Outlet Check (Verify Mix):\")\n",
        "print(df['website'].value_counts().head())\n",
        "\n",
        "\n",
        "#  3. CLEANING\n",
        "print(\"\\n PHASE 2: PROCESSING \")\n",
        "def clean_text(text_obj):\n",
        "    text = \" \".join(text_obj) if isinstance(text_obj, list) else str(text_obj)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "    words = text.split()\n",
        "    clean_words = [w for w in words if w not in final_stop_list and len(w) > 3]\n",
        "    return \" \".join(clean_words)\n",
        "\n",
        "df['clean_text'] = df['body'].apply(clean_text)\n",
        "\n",
        "#  4. HYBRID TOPIC MODELING (The Logic Fix)\n",
        "print(\"\\n PHASE 3: HYBRID AI TRAINING \")\n",
        "# Strategy: Train Brain on Balanced Subset -> Predict on Full Dataset\n",
        "training_df = df.groupby('website').apply(\n",
        "    lambda x: x.sample(n=min(len(x), 3000), random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(f\"üß† Training Vectorizer on {len(training_df):,} balanced articles...\")\n",
        "vectorizer = TfidfVectorizer(max_df=0.90, min_df=5, max_features=5000, stop_words=final_stop_list)\n",
        "tfidf_train = vectorizer.fit_transform(training_df['clean_text'])\n",
        "\n",
        "print(\"üß† Training Topic Model...\")\n",
        "nmf_model = NMF(n_components=12, random_state=42, init='nndsvd')\n",
        "nmf_model.fit(tfidf_train)\n",
        "\n",
        "# Label Topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "topic_labels = {}\n",
        "for i, topic in enumerate(nmf_model.components_):\n",
        "    top_indices = topic.argsort()[:-4:-1]\n",
        "    top_words = [feature_names[j].upper() for j in top_indices]\n",
        "    topic_labels[i] = \" | \".join(top_words)\n",
        "\n",
        "print(\"üöÄ Applying Topic Model to FULL Dataset...\")\n",
        "tfidf_full = vectorizer.transform(df['clean_text'])\n",
        "df['topic_id'] = nmf_model.transform(tfidf_full).argmax(axis=1)\n",
        "df['topic_label'] = df['topic_id'].map(topic_labels)\n",
        "\n",
        "#  5. ENTITY INTELLIGENCE (Locations & Personas)\n",
        "print(\"\\n PHASE 4: ENTITY RESOLUTION \")\n",
        "\n",
        "def resolve_entities(text_obj):\n",
        "    text = \" \".join(text_obj) if isinstance(text_obj, list) else str(text_obj)\n",
        "    matches = re.findall(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)', text)\n",
        "\n",
        "    found_locs = set()\n",
        "    potential_people = []\n",
        "\n",
        "    blacklist = {'Abante', 'Tonite', 'News', 'Photo', 'Source', 'Courtesy'}\n",
        "\n",
        "    for m in matches:\n",
        "        m_lower = m.lower()\n",
        "        if len(m) < 4 or m in blacklist: continue\n",
        "\n",
        "        # Check Location DB\n",
        "        if m_lower in loc_db:\n",
        "            data = loc_db[m_lower]\n",
        "            # Output: \"City Name (Region)\"\n",
        "            found_locs.add(f\"{data['official_name']} ({data['region']})\")\n",
        "        else:\n",
        "            potential_people.append(m)\n",
        "\n",
        "    return list(found_locs), potential_people\n",
        "\n",
        "print(\"Extracting & Mapping Locations...\")\n",
        "results = df['body'].apply(resolve_entities)\n",
        "df['locations'] = results.apply(lambda x: x[0])\n",
        "df['raw_entities'] = results.apply(lambda x: x[1])\n",
        "\n",
        "#  6. UNSUPERVISED PERSONA CLUSTERING\n",
        "print(\"üß† Normalizing Personas (Fuzzy Logic)...\")\n",
        "# 1. Count frequency of all \"potential people\"\n",
        "all_people = [p for sublist in df['raw_entities'] for p in sublist]\n",
        "from collections import Counter\n",
        "# 2. Identify \"Anchors\" (Top 200 most frequent names)\n",
        "top_anchors = [x[0] for x in Counter(all_people).most_common(200)]\n",
        "\n",
        "def normalize_personas(name_list):\n",
        "    normalized = set()\n",
        "    for name in name_list:\n",
        "        # Check against Anchors\n",
        "        # score_cutoff=90 means very high similarity required\n",
        "        match = process.extractOne(name, top_anchors, scorer=fuzz.token_set_ratio, score_cutoff=90)\n",
        "        if match:\n",
        "            normalized.add(match[0]) # Map to Anchor\n",
        "        else:\n",
        "            if \" \" in name: normalized.add(name)\n",
        "\n",
        "    return list(normalized)[:5]\n",
        "\n",
        "df['personas'] = df['raw_entities'].apply(normalize_personas)\n",
        "\n",
        "#  7. EXPORT & DATE REPAIR\n",
        "print(\"\\n PHASE 5: FINALIZING & DATE REPAIR \")\n",
        "\n",
        "def simple_sentiment(text):\n",
        "    blob = TextBlob(text).sentiment.polarity\n",
        "    if 'hindi' in text and blob > 0: return -blob\n",
        "    return blob\n",
        "\n",
        "df['sentiment'] = df['clean_text'].apply(simple_sentiment)\n",
        "\n",
        "# 1. Try strict parsing for Abante format (@)\n",
        "df['date_clean'] = pd.to_datetime(df['date'].str.split('@').str[0], errors='coerce')\n",
        "\n",
        "# 2. Check for failures (NaT)\n",
        "missing_mask = df['date_clean'].isna()\n",
        "failed_count = missing_mask.sum()\n",
        "\n",
        "if failed_count > 0:\n",
        "    print(f\"‚ö†Ô∏è Warning: {failed_count:,} articles had invalid date formats.\")\n",
        "    print(\"üõ†Ô∏è Applying Repair: Filling invalid dates with the median date of the dataset.\")\n",
        "\n",
        "    # 3. Fill invalid dates with the Median (Middle) date so they don't get deleted\n",
        "    median_date = df['date_clean'].median()\n",
        "    df.loc[missing_mask, 'date_clean'] = median_date\n",
        "\n",
        "# Select Columns\n",
        "df_final = df[[\n",
        "    'title', 'date_clean', 'category', 'website',\n",
        "    'topic_label', 'sentiment', 'personas', 'locations', 'url', 'clean_text'\n",
        "]]\n",
        "\n",
        "output_file = 'tala_final.parquet'\n",
        "df_final.to_parquet(output_file)\n",
        "print(f\"\\n‚úÖ PIPELINE SUCCESS. Saved to {output_file}\")\n",
        "print(\"Final Outlet Verification (Should match Raw):\")\n",
        "print(df_final['website'].value_counts())"
      ],
      "metadata": {
        "id": "7eRl4HSXupeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# C). EXPORT TO DRIVE\n",
        "# ==============================================\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Copy the file to your \"My Drive\" root folder\n",
        "source = '/content/tala_final.parquet'\n",
        "destination = '/content/drive/MyDrive/tala_final.parquet'\n",
        "\n",
        "try:\n",
        "    shutil.copy(source, destination)\n",
        "    print(f\"‚úÖ SUCCESS! File saved to your Google Drive at: {destination}\")\n",
        "    print(\"Go to drive.google.com to download it safely.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: Could not find 'tala_final.parquet'. Did the previous step finish?\")"
      ],
      "metadata": {
        "id": "uF3BSuaZutQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}